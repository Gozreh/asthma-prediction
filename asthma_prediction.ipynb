{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the daily prompt survey into pandas. Group by patients and label each generated 7-day data window according to whether they contain an amber-zone. All 7 days in each window are labelled the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = pd.read_csv('data/Daily prompt survey.csv').drop(['ROW_ID','ROW_VERSION','recordId','appVersion','phoneInfo'],axis=1)\n",
    "daily['get_worse_counts']=daily['get_worse'].str.count('\\d+')   # sum total number of asthma triggers\n",
    "daily['unix_time'] = daily['createdOn'] # convert unix time to date format\n",
    "daily['createdOn'] = pd.to_datetime(daily['createdOn'], unit='ms').dt.normalize()   # normalise date format\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "data = list()\n",
    "\n",
    "# group by patient (healthCode)\n",
    "for key, grp in daily.groupby(['healthCode']):\n",
    "    if len(grp)>49:     # select patients with 50+ entries\n",
    "\n",
    "        # split patient data timeframe into windows of 7 days and bin patient data entries accordingly\n",
    "        interval_range = pd.interval_range(start=grp['createdOn'].iloc[0], freq='7D', end=grp['createdOn'].iloc[-1]+pd.DateOffset(7), closed='left')\n",
    "        grp['bin'] = pd.cut(grp['createdOn'], bins=interval_range)\n",
    "\n",
    "        pfmax = np.nanmax(grp['peakflow'])  # used for peak flow normalisation\n",
    "        \n",
    "        # group all data entries per window (still per patient)\n",
    "        for x, y in grp.groupby(['bin']):\n",
    "\n",
    "            # label amber-zones using criteria below\n",
    "            night = y['night_symptoms'].any()\n",
    "            qr = (y['quick_relief_puffs']>=3).any()\n",
    "            y['PEF'] = y['peakflow']/pfmax\n",
    "            pf = (y['peakflow']<pfmax*0.7).any()\n",
    "            \n",
    "            label = 0\n",
    "            if night and pf:\n",
    "                label=1\n",
    "            if qr:\n",
    "                label=1\n",
    "            \n",
    "            y['label']=label\n",
    "\n",
    "            data.append(y)\n",
    "daily_label = pd.concat(data, ignore_index=True)\n",
    "\n",
    "daily_label.loc[daily_label['use_qr']==False, 'quick_relief_puffs']=0\n",
    "\n",
    "\n",
    "\n",
    "daily_label.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data labelled, feature extraction is next. This involves making a feature window 7 days before the labelled window until the start of the labelled window. Features are extracted from this new window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list()\n",
    "linreg = LinearRegression()    # define model for finding gradients later\n",
    "\n",
    "# group by patient\n",
    "for key, grp in daily_label.groupby(['healthCode']):\n",
    "\n",
    "        # group by labelled window\n",
    "        for x,y in grp.groupby(['bin']):\n",
    "            start = x.left-pd.DateOffset(7) # find date 7 days before labelled window\n",
    "            end = x.left    # find date at start of labelled window\n",
    "            label = y['label'].mean()   # find label\n",
    "            intermed = grp[(grp.createdOn < end) & (start <= grp.createdOn)]    # define feature window using start and end dates and capture all relevant data entries\n",
    "\n",
    "            # filter feature windows by number of entries. All fields must contain >2 entries\n",
    "            if len(intermed)>1 and intermed['quick_relief_puffs'].count()>2 and intermed['day_symptoms'].count()>2 and intermed['night_symptoms'].count()>2 and intermed['get_worse_counts'].count()>2 and intermed['peakflow'].count()>2:\n",
    "                \n",
    "                # create mask to filter out NaN values as linreg cannot handle these. Apply mask to all feature fields\n",
    "                mask = ~np.isnan(intermed['unix_time']) & ~np.isnan(intermed['quick_relief_puffs']) & ~np.isnan(intermed['get_worse_counts']) & ~np.isnan(intermed['PEF'])\n",
    "                unix = (intermed['unix_time']/10**10)[mask].values.reshape(-1,1)\n",
    "                qr = intermed['quick_relief_puffs'][mask].values.reshape(-1,1)\n",
    "                worse = intermed['get_worse_counts'][mask].values.reshape(-1,1)\n",
    "                pef = intermed['PEF'][mask].values.reshape(-1,1)\n",
    "\n",
    "                # extract features from fields. Use linreg for finding gradient\n",
    "                qr_mean = intermed['quick_relief_puffs'].mean()\n",
    "                qr_grad = linreg.fit(unix,qr).coef_[0][0]   # returns gradient\n",
    "                qr_abs = abs(qr_grad)\n",
    "                \n",
    "\n",
    "                day_mean = (intermed['day_symptoms']==True).mean()  # sum number of entries with day symptoms = true\n",
    "                night_mean = (intermed['night_symptoms']==True).mean()\n",
    "\n",
    "\n",
    "                worse_mean = intermed['get_worse_counts'].mean()\n",
    "                worse_grad = linreg.fit(unix,worse).coef_[0][0]\n",
    "                worse_abs = abs(worse_grad)\n",
    "\n",
    "                \n",
    "                pef_mean = intermed['PEF'].mean()\n",
    "                pef_grad = linreg.fit(unix,pef).coef_[0][0]\n",
    "                pef_abs = abs(pef_grad)\n",
    "\n",
    "                total = len(intermed)   # total number of entries in feature window\n",
    "\n",
    "                data.append((key,x,qr_mean,qr_grad,qr_abs,day_mean,night_mean,worse_mean,worse_grad,worse_abs,pef_mean,pef_grad,pef_abs,total,label))\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "# place results into new dataframe\n",
    "daily_features = pd.DataFrame(data)\n",
    "daily_features.columns = ['user','date range','qr_mean','qr_grad','qr_abs','day_mean','night_mean','worse_mean','worse_grad','worse_abs','pef_mean','pef_grad','pef_abs','total','label']\n",
    "print(daily_features.shape)\n",
    "print(daily_features['label'].sum())\n",
    "daily_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the features are ranked on their importance to prediction. A forward feature selector using a logistic regression classifier and a maximum relevance-minimum redundancy algorithm are used. MRMR is used in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# select columns containing the features and split into a test and validation selection\n",
    "feature_cols = ['qr_mean','qr_grad','qr_abs','day_mean','night_mean','worse_mean','worse_grad','worse_abs','pef_mean','pef_grad','pef_abs','total']\n",
    "\n",
    "X = daily_features.loc[:,feature_cols]\n",
    "y = daily_features.label\n",
    "\n",
    "# split into training and validation subsets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3, shuffle=True)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# select model for sfs\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# define sequential feature selector (sfs)\n",
    "sfs = SFS(logreg,\n",
    "          k_features=11, # want to rank all features ( k = max total features -1)\n",
    "          forward=True, # want selection direction to be forward (add features)\n",
    "          floating=False,\n",
    "          scoring = 'f1',\n",
    "          cv = 0)\n",
    "\n",
    "# fit sfs model to data\n",
    "sfs.fit(X_train, y_train, custom_feature_names=feature_cols)\n",
    "\n",
    "# retrieve feature scores\n",
    "features_df=pd.DataFrame.from_dict(sfs.get_metric_dict()).T\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MRMR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "# inputs:\n",
    "#    X: pandas.DataFrame, features\n",
    "#    y: pandas.Series, target variable\n",
    "#    K: number of features to select\n",
    "\n",
    "# compute F-statistics and initialize correlation matrix\n",
    "F = pd.Series(f_regression(X, y)[0], index = X.columns)\n",
    "corr = pd.DataFrame(.00001, index = X.columns, columns = X.columns)\n",
    "\n",
    "# initialize list of selected features and list of excluded features\n",
    "selected = []\n",
    "not_selected = X.columns.to_list()\n",
    "score_list = []\n",
    "\n",
    "# repeat K times\n",
    "for i in range(10):\n",
    "  \n",
    "    # compute (absolute) correlations between the last selected feature and all the (currently) excluded features\n",
    "    if i > 0:\n",
    "        last_selected = selected[-1]\n",
    "        corr.loc[not_selected, last_selected] = X[not_selected].corrwith(X[last_selected]).abs().clip(.00001)\n",
    "        \n",
    "    # compute FCQ score for all the (currently) excluded features (this is Formula 2)\n",
    "    score = F.loc[not_selected] / corr.loc[not_selected, selected].mean(axis = 1).fillna(.00001)\n",
    "    \n",
    "    # find best feature, add it to selected and remove it from not_selected\n",
    "    best = score.index[score.argmax()]\n",
    "    selected.append(best)\n",
    "    not_selected.remove(best)\n",
    "    score_list.append(score[score.argmax()])\n",
    "\n",
    "print(selected)\n",
    "print(score_list)\n",
    "F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 5 different classifiers are tested on the data. GridsearchCV is used to find the best hyperparameters for the models. Repeated CV is then used to evaluate the models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import model_selection\n",
    "\n",
    "feature_cols = ['qr_mean','day_mean','night_mean','worse_mean','pef_mean','total']  # select features to be taken forward\n",
    "\n",
    "X = daily_features.loc[:,feature_cols]\n",
    "y = daily_features.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters to be tested are defined per classifier. Trialling is repeated 5 times. The scoring method used is f1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'C':range(1,5,1),'gamma':range(1,5,1)}    # define params\n",
    "\n",
    "grid = model_selection.GridSearchCV(SVC(),parameters, scoring = 'f1',cv=5)\n",
    "\n",
    "grid.fit(X,y)\n",
    "\n",
    "grid_svm = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators':range(10,160,10),'max_features':['sqrt','log2',6]}\n",
    "grid = model_selection.GridSearchCV(RandomForestClassifier(),parameters, scoring = 'f1',cv=5)\n",
    "\n",
    "grid.fit(X,y)\n",
    "\n",
    "grid_rf = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_neighbors':range(1,10),'metric':['euclidean', 'manhattan', 'minkowski'],'p':[1,2]}\n",
    "grid = model_selection.GridSearchCV(KNeighborsClassifier(),parameters, scoring = 'f1',cv=5)\n",
    "\n",
    "grid.fit(X,y)\n",
    "\n",
    "grid_knn = pd.DataFrame(grid.cv_results_)\n",
    "\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the optimal hyperparameters are found, the models are tested. Here 5x5 CVs are used to validate each model. Results are tabulated and displayed as boxplots. 6 evaluation metrics are used. ROCs are also calulated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define models with relevant hyperparameters\n",
    "models = [\n",
    "          ('LogReg', LogisticRegression()), \n",
    "          ('RF', RandomForestClassifier(n_estimators=130, max_features='log2')),\n",
    "          ('kNN', KNeighborsClassifier(n_neighbors=9,metric='euclidean',p=1)),\n",
    "          ('SVM', SVC(C=1,gamma=1)), \n",
    "          ('GNB', GaussianNB())\n",
    "        ]\n",
    "\n",
    "# initiate trackers\n",
    "names = []\n",
    "dfs = []\n",
    "dfs1 = []\n",
    "\n",
    "# define scoring/evaluation metrics\n",
    "scoring = {'accuracy':'accuracy', \n",
    "          'precision':'precision', \n",
    "          'recall':'recall', \n",
    "          'specificity': make_scorer(recall_score,pos_label=0), # specificity scorer has to to seperately defined as it is not standard\n",
    "          'f1':'f1', \n",
    "          'roc_auc':'roc_auc'\n",
    "          }\n",
    "\n",
    "# perform repeated CV on models\n",
    "for name, model in models:\n",
    "    kfold = model_selection.RepeatedKFold(n_splits=5, n_repeats=5)  # define CV folds\n",
    "    cv_results = model_selection.cross_validate(model, X, y, cv=kfold, scoring=scoring) # perform CV and return results\n",
    "\n",
    "    # place results temporarily into pandas df as we go along and calculate median\n",
    "    names.append(name)\n",
    "    df = pd.DataFrame(cv_results)\n",
    "    median = pd.DataFrame(df.median().to_dict(),index=[df.index.values[-1]])\n",
    "    df['model']=name\n",
    "    median['model']=name\n",
    "    dfs.append(df)\n",
    "    dfs1.append(median)\n",
    "\n",
    "# concat median results\n",
    "final = pd.concat(dfs, ignore_index=True)\n",
    "final_median = pd.concat(dfs1, ignore_index=True)\n",
    "final_median\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "# define figure properties\n",
    "rc = {'axes.facecolor':'white',\n",
    "      'axes.grid' : True,\n",
    "      'grid.color': '.8',\n",
    "      'font.family':'Times New Roman',\n",
    "      'font.size' : 15}\n",
    "plt.rcParams.update(rc)\n",
    "\n",
    "# process data so seaborn can use it\n",
    "final_long = final.drop(['fit_time','score_time'],axis=1)\n",
    "final_long.columns = ['Accuracy','Precision','Recall/Sensitivity','Specificity','F1-score','AUC','Model']\n",
    "df_long = pd.melt(final_long,'Model', var_name='Evaluation Metric', value_name='Score')   # melting stage is important\n",
    "\n",
    "# plot data\n",
    "sns.catplot('Evaluation Metric', hue='Model', y='Score', data=df_long, kind='box', height=5, aspect=2.5)\n",
    "sns.despine(fig=None, ax=None, top=False, right=False, left=False, bottom=False, offset=None, trim=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate mean ROC curves for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "colour = ['b','r','y','g','m']\n",
    "j=0\n",
    "\n",
    "# define CV splits\n",
    "cv1 = StratifiedKFold(n_splits=5, shuffle=True)\n",
    "\n",
    "# convert to numpy\n",
    "X_num = X.to_numpy()\n",
    "y_num = y.to_numpy()\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "fig, ax = plt.subplots(figsize=(9,7))\n",
    "\n",
    "for name, model in models:\n",
    "\n",
    "    # initiate trackers\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # perform CV on models, but this time create ROC curves\n",
    "    for i, (train, test) in enumerate(cv1.split(X_num, y_num)):\n",
    "        model.fit(X_num[train], y_num[train])\n",
    "        viz = RocCurveDisplay.from_estimator(\n",
    "            model,\n",
    "            X_num[test],\n",
    "            y_num[test],\n",
    "            name=None,\n",
    "            alpha=0,\n",
    "            lw=1,\n",
    "            ax=ax1,     # place each individual CV result onto throw-away figure (simple workaround having all results on one plot)\n",
    "        )\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    \n",
    "    # calculate mean ROC\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "\n",
    "    # plot mean ROCs on actual plot\n",
    "    ax.plot(\n",
    "        mean_fpr,\n",
    "        mean_tpr,\n",
    "        label=\"{}\".format(name)+r' (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc,std_auc),\n",
    "        color=colour[j],\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    j+=1\n",
    "\n",
    "# style plot\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"k\", label=\"Chance\", alpha=0.8)    \n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ecdede3ee117fce16f9afedacb69f17c8a73fd558902dc318e1c4ab38a2b2131"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
